# -*- coding: utf-8 -*-
"""Elaborato_finale

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WoT9REPjl_LNkG0xmwtRGELPggu9892R
"""

import os
os.environ["TRANSFORMERS_NO_TF"] = "1"
!pip install --quiet optuna
!pip install --quiet emoji
import pandas as pd
import string
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score
from nltk.corpus import stopwords
import optuna
import re
import emoji
import matplotlib.pyplot as plt
import joblib

from google.colab import drive
drive.mount('/gdrive')

data_train= open('/gdrive/MyDrive/Elaborato_finale/Data/AMI2020_training_raw_anon.tsv')
data_test= open('/gdrive/MyDrive/Elaborato_finale/Data/AMI2020_test_raw_gold_anon.tsv')

nltk.download('stopwords')
stop_words = set(stopwords.words('italian'))

def preprocess(text):
    text = text.lower()
    text = re.sub(r"<[^>]+>", "", text)
    text = re.sub(r'&#x[0-9a-fA-F]+;', '', text)
    text = re.sub(r'&#\d+;', '', text)
    text = emoji.replace_emoji(text, replace='')
    text = ''.join([c for c in text if c not in string.punctuation])
    text = re.sub(r'#', '', text)
    tokens = text.split()
    tokens = [word for word in tokens if word not in stop_words]
    return ' '.join(tokens)

import csv
data= csv.reader (open('/gdrive/MyDrive/Elaborato_finale/Data/AMI2020_training_raw_anon.tsv'))
data= [i for i in data]
data[150]

righe = []

with open("/gdrive/MyDrive/Elaborato_finale/Data/AMI2020_training_raw_anon.tsv", encoding="utf-8") as f:
    for line_num, line in enumerate(f):
        parts = line.strip().split('\t')
        i = 0
        while i + 3 < len(parts):
            id_ = parts[i].strip()
            text = parts[i + 1].strip()
            misogynous = parts[i + 2].strip()
            aggressiveness = parts[i + 3].strip()

            if misogynous in ['0', '1'] and aggressiveness in ['0', '1']:
                righe.append({
                    'id': id_,
                    'text': text,
                    'misogynous': int(misogynous),
                    'aggressiveness': int(aggressiveness)
                })

            i += 4

df_train = pd.DataFrame(righe)
df_train

df_test= pd.read_csv('/gdrive/MyDrive/Elaborato_finale/Data/AMI2020_test_raw_gold_anon.tsv', sep= "\t")
df_test

df_train['text'] = df_train['text'].apply(preprocess)
df_train['text']

df_test['text'] = df_test['text'].apply(preprocess)
df_test['text']

X_train = df_train['text']
y_train = df_train['misogynous']

X_test = df_test['text']
y_test = df_test['misogynous']

#Logistic Regression
def objective_lr(trial):
    C = trial.suggest_float("lr_c", 1e-4, 100.0, log=True)
    model = LogisticRegression(C=C, max_iter=1000)

    pipe = Pipeline([
        ('tfidf', TfidfVectorizer(max_features=5000)),
        ('clf', model)
    ])
    pipe.fit(X_train, y_train)
    preds = pipe.predict(X_test)
    return accuracy_score(y_test, preds)

#SVC
def objective_svc(trial):
    C = trial.suggest_float("svc_c", 1e-3, 100.0, log=True)
    kernel = trial.suggest_categorical("kernel", ["linear", "rbf"])
    gamma = trial.suggest_float("gamma", 1e-4, 1e-1, log=True)
    model = SVC(C=C, kernel=kernel, gamma=gamma)

    pipe = Pipeline([
        ('tfidf', TfidfVectorizer(max_features=5000)),
        ('clf', model)
    ])
    pipe.fit(X_train, y_train)
    preds = pipe.predict(X_test)
    return accuracy_score(y_test, preds)

# Random Forest
def objective_rf(trial):
    n_estimators = trial.suggest_int("n_estimators", 50, 300)
    max_depth = trial.suggest_int("max_depth", 3, 30)
    max_features = trial.suggest_categorical("max_features", ["sqrt", "log2", None])
    model = RandomForestClassifier(n_estimators=n_estimators,max_depth=max_depth, max_features=max_features, random_state=42)

    pipe = Pipeline([
        ('tfidf', TfidfVectorizer(max_features=5000)),
        ('clf', model)
    ])
    pipe.fit(X_train, y_train)
    preds = pipe.predict(X_test)
    return accuracy_score(y_test, preds)

print("Ottimizzazione Logistic Regression...")
study_lr = optuna.create_study(direction="maximize")
study_lr.optimize(objective_lr, n_trials=100)

print("Ottimizzazione SVC...")
study_svc = optuna.create_study(direction="maximize")
study_svc.optimize(objective_svc, n_trials=100)

print("Ottimizzazione Random Forest...")
study_rf = optuna.create_study(direction="maximize")
study_rf.optimize(objective_rf, n_trials=100)

final_scores = {
    "Logistic Regression": study_lr.best_value,
    "SVC": study_svc.best_value,
    "Random Forest": study_rf.best_value
}

#Grafico
plt.figure(figsize=(8,5))
plt.bar(final_scores.keys(), final_scores.values(), color=['blue', 'green', 'orange'])
plt.ylim(0, 1)
plt.ylabel("Accuracy")
plt.title("Confronto finale tra modelli")
plt.grid(True)
plt.show()

print(f"Logistic Regression\n - Accuracy: {study_lr.best_value:.4f}\n - Parametri: {study_lr.best_params}\n")
print(f"SVC\n - Accuracy: {study_svc.best_value:.4f}\n - Parametri: {study_svc.best_params}\n")
print(f"Random Forest\n - Accuracy: {study_rf.best_value:.4f}\n - Parametri: {study_rf.best_params}\n")

best_model_name = max(final_scores, key=final_scores.get)
best_score = final_scores[best_model_name]

print("Miglior modello complessivo:")
print(f"{best_model_name} con Accuracy = {best_score:.4f}")

def mc(model_name, model, X_train, y_train, X_test, y_test):
    pipe = Pipeline([
        ('tfidf', TfidfVectorizer(max_features=5000)),
        ('clf', model)
    ])
    pipe.fit(X_train, y_train)
    y_pred = pipe.predict(X_test)

    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(cmap="Blues")
    plt.title(f"Matrice di Confusione â€“ {model_name}")
    plt.grid(False)
    plt.show()
    print(f"Accuracy {model_name}: {accuracy_score(y_test, y_pred):.4f}")

params_lr = study_lr.best_params
model_lr = LogisticRegression(C=params_lr["lr_c"], max_iter=1000)
mc("Logistic Regression", model_lr, X_train, y_train, X_test, y_test)

params_svc = study_svc.best_params
model_svc = SVC(C=params_svc["svc_c"], kernel=params_svc["kernel"], gamma=params_svc["gamma"])
mc("SVC", model_svc, X_train, y_train, X_test, y_test)

params_rf = study_rf.best_params
model_rf = RandomForestClassifier(
    n_estimators=params_rf["n_estimators"],
    max_depth=params_rf["max_depth"],
    max_features=params_rf["max_features"],
    random_state=42
)
mc("Random Forest", model_rf, X_train, y_train, X_test, y_test)

save_dir = "/gdrive/MyDrive/Elaborato_finale/modelli_misoginia"
os.makedirs(save_dir, exist_ok=True)
if best_model_name == "Logistic Regression":
    params = study_lr.best_params
    model = LogisticRegression(C=params["lr_c"], max_iter=1000)

elif best_model_name == "SVC":
    params = study_svc.best_params
    model = SVC(C=params["svc_c"], kernel=params["kernel"], gamma=params["gamma"])

else:
    params = study_rf.best_params
    model = RandomForestClassifier(
        n_estimators=params["n_estimators"],
        max_depth=params["max_depth"],
        max_features=params["max_features"],
        random_state=42
    )
pipeline_finale = Pipeline([
    ('tfidf', TfidfVectorizer(max_features=3000)),
    ('clf', model)
])
pipeline_finale.fit(X_train, y_train)

file_modello = os.path.join(save_dir, f"Modello_{best_model_name.replace(' ', '_')}.joblib")
joblib.dump(pipeline_finale, file_modello)

test_label= df_test['misogynous'].tolist()
test_text= df_test['text'].tolist()
train_label= df_train['misogynous'].tolist()
train_text= df_train['text'].tolist()

import os
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments,  DataCollatorWithPadding
import torch
from torch.utils.data import Dataset
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

save_dir = "/gdrive/MyDrive/Elaborato_finale/modelli_misoginia"
os.makedirs(save_dir, exist_ok=True)

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-multilingual-cased")

train_encodings = tokenizer(train_text, truncation=True, padding=True)
test_encodings = tokenizer(test_text, truncation=True, padding=True)

class CustomDataset(Dataset):
  def __init__(self, encodings, labels):
    self.encodings = encodings
    self.labels = labels

  def __getitem__(self, idx):
    item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}
    item["labels"] = torch.tensor(self.labels[idx])
    return item

  def __len__(self):
    return len(self.labels)

train_dataset = CustomDataset (train_encodings, train_label)
test_dataset = CustomDataset (test_encodings, test_label)

model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-multilingual-cased", num_labels=2)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

training_args = TrainingArguments(
    output_dir= "./results" ,
    learning_rate=1e-5,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    num_train_epochs=5,
    weight_decay=0.01,
    eval_strategy= "epoch" ,
    logging_strategy= "epoch",
    report_to="none"
)

trainer = Trainer (
  model=model,
  args=training_args,
  train_dataset=train_dataset,
  eval_dataset=test_dataset,
  tokenizer=tokenizer,
  data_collator=data_collator,
)

trainer.train()

predictions = trainer.predict(test_dataset)
preds = torch.argmax(torch.tensor(predictions.predictions), dim=1)
print("Classification Report:")
print(classification_report(predictions.label_ids, preds.numpy()))

cm = confusion_matrix(predictions.label_ids, preds.numpy())
plt.figure(figsize=(5, 4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Non Misogino", "Misogino"], yticklabels=["Non Misogino", "Misogino"])
plt.xlabel("Predizione")
plt.ylabel("Reale")
plt.title("Matrice di Confusione - DistilBERT")
plt.show()

model_path = os.path.join(save_dir, "distilbert_model")
model.save_pretrained(model_path)
tokenizer.save_pretrained(model_path)

!pip install optuna
import optuna
import os
import fasttext
!pip install numpy==1.26.4
!pip install fasttext==0.9.2
import numpy as np
import seaborn as sns
from sklearn.metrics import f1_score

save_dir = "/gdrive/MyDrive/Elaborato_finale/modelli_misoginia"
os.makedirs(save_dir, exist_ok=True)

def save_fasttext_format(df, text_col, label_col, filename):
  with open(filename, 'w', encoding='utf-8') as f:
      for _, row in df.iterrows():
        label = f"__label__{int(row['misogynous'])}"
        text = str(row["text"]).replace("\n", " ").strip()
        if text:
          f.write(f"{label} {text}\n")

save_fasttext_format(df_train, "text", "misogynous", "train_ft.txt")
save_fasttext_format(df_test, "text", "misogynous", "test_ft.txt")

def objective(trial):
    lr = trial.suggest_float("lr", 0.005, 1.0, log=True)
    epoch = trial.suggest_int("epoch", 5, 200)
    wordNgrams = trial.suggest_int("wordNgrams", 3, 10)
    dim = trial.suggest_categorical("dim", [50, 100, 150, 200])
    try:
      model = fasttext.train_supervised(input="train_ft.txt", lr=lr, epoch=epoch, wordNgrams=wordNgrams, dim=dim)
    except RuntimeError:
      return 0.0

    y_true = []
    y_pred = []

    for _, row in df_test.iterrows():
        text = str(row["text"]).replace("\n", " ").strip()
        true_label = int(row["misogynous"])
        if text:
          pred_label = model.predict(text, k=1)[0][0]
          pred_label = int(pred_label.replace("__label__", ""))
          y_true.append(true_label)
          y_pred.append(pred_label)
    return f1_score(y_true, y_pred)

study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=100)

print("Migliori iperparametri trovati:", study.best_params)
print(f"Accuracy: {study.best_value:.4f}")

best_params = study.best_params
final_model = fasttext.train_supervised(
    input="train_ft.txt",
    lr=best_params["lr"],
    epoch=best_params["epoch"],
    wordNgrams=best_params["wordNgrams"],
    dim=best_params["dim"]
)

y_true = []
y_pred = []

for _, row in df_test.iterrows():
    text = str(row["text"]).replace("\n", " ").strip()
    label = int(row["misogynous"])
    if text:
      pred_label = final_model.predict(text, k=1)[0][0]
      pred_label = int(pred_label.replace("__label__", ""))
      y_true.append(label)
      y_pred.append(pred_label)

print("Classification Report:")
from sklearn.metrics import classification_report
print(classification_report(y_true, y_pred))
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(5, 4))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=["Non Misogino", "Misogino"],
            yticklabels=["Non Misogino", "Misogino"])
plt.xlabel("Predizione")
plt.ylabel("Reale")
plt.title("Matrice di Confusione - FastText")
plt.show()

model_path = os.path.join(save_dir, "fasttext_model.bin")
final_model.save_model(model_path)

!pip install openpyxl
!pip install numpy==1.26.4
!pip install fasttext==0.9.2
import pandas as pd
from google.colab import drive
import fasttext
import numpy as np
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
import torch
import joblib
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

dataset_path = "/gdrive/MyDrive/Elaborato_finale/Data/implicit_misogyny_idtext.xlsx"
fasttext_model_path = "/gdrive/MyDrive/Elaborato_finale/modelli_misoginia/fasttext_model.bin"
distilbert_model_path = "/gdrive/MyDrive/Elaborato_finale/modelli_misoginia/distilbert_model"
pipeline_model_path = "/gdrive/MyDrive/Elaborato_finale/modelli_misoginia/Modello_Logistic_Regression.joblib"

df = pd.read_excel(dataset_path)
df["text"] = df["text"].fillna("")
texts = df["text"].tolist()
labels = [1] * len(texts)

ft_model = fasttext.load_model(fasttext_model_path)
preds_fasttext = []
for t in texts:
    t_clean = str(t).replace("\n", " ").strip()
    if t_clean:
        label, _ = ft_model.predict(t_clean)
        label_num = int(label[0].replace("__label__", ""))
        preds_fasttext.append(label_num)
    else:
        preds_fasttext.append(None)

tokenizer = DistilBertTokenizer.from_pretrained(distilbert_model_path)
bert_model = DistilBertForSequenceClassification.from_pretrained(distilbert_model_path)
bert_model.eval()
preds_bert = []
for t in texts:
    if t.strip():
        inputs = tokenizer(t, truncation=True, padding=True, return_tensors="pt")
        with torch.no_grad():
            outputs = bert_model(**inputs)
        pred_class = outputs.logits.argmax(dim=1).item()
        preds_bert.append(pred_class)
    else:
        preds_bert.append(None)

pipeline_model = joblib.load(pipeline_model_path)
preds_logreg = pipeline_model.predict(texts).tolist()

def evaluate_model(name, y_true, y_pred):
    print(f"\n Risultati {name}")
    print(classification_report(y_true, y_pred))
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                xticklabels=["Non Misogino", "Misogino"],
                yticklabels=["Non Misogino", "Misogino"])
    plt.title(f"Matrice di Confusione - {name}")
    plt.xlabel("Predetto")
    plt.ylabel("Reale")
    plt.show()

if labels is not None:
    evaluate_model("FastText", labels, preds_fasttext)
    evaluate_model("DistilBERT", labels, preds_bert)
    evaluate_model("Logistic Regression", labels, preds_logreg)
else:
    print("Nessuna etichetta trovata: calcolo solo le predizioni.")